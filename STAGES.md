# Research Paper POC - Detailed Stages

Complete breakdown of the 5-stage workflow for transforming research papers into working prototypes. Each stage includes specific objectives, AI collaboration patterns, deliverables, and validation checkpoints.

## ðŸŽ¯ Stage Overview

```
Stage â†’ High-Level Goal â†’ AI Prompts â†’ Tasks â†’ Tickets â†’ Checkpoints
```

Each stage follows the universal structure with research-specific adaptations:
- **Entry Criteria:** What must be complete before starting
- **AI Collaboration:** Specific prompts and interaction patterns  
- **Tasks:** Concrete work items generated by AI
- **Deliverables:** Tangible outputs that enable next stage
- **Exit Criteria:** Validation checkpoints before proceeding

---

# Stage 1: Problem Discovery
**Duration:** 1-2 hours  
**AI Persona:** Research Analyst  
**Primary Goal:** Determine if paper can be implemented as POC within reasonable constraints

## Entry Criteria
- [ ] Research paper identified and accessible
- [ ] High-level understanding of paper's domain
- [ ] Basic development environment available
- [ ] Time allocated for POC project (1-3 weeks)

## High-Level Objective
Perform rapid triage and feasibility assessment to make an informed go/no-go decision before investing significant time in implementation.

## AI Collaboration Pattern: High-Autonomy Analysis

### Primary AI Prompt
```markdown
**Role:** You are a senior research engineer with expertise in [DOMAIN]. 

**Context:** I want to implement a proof-of-concept based on this research paper: [PAPER_TITLE] by [AUTHORS].

**Paper Summary:** [ABSTRACT_AND_KEY_SECTIONS]

**Task:** Perform a comprehensive feasibility analysis for implementing this paper as a POC.

**Analysis Framework:**
1. **Algorithm Clarity:** How well-described is the core algorithm?
2. **Data Requirements:** What datasets are needed and are they accessible?
3. **Computational Complexity:** What are the resource requirements?
4. **Dependencies:** What libraries, frameworks, or tools are required?
5. **Reproducibility:** How much implementation guidance is provided?

**Output Format:**
- **Core Algorithm Summary:** [2-3 sentences describing the main contribution]
- **Implementation Complexity:** [Simple/Medium/Complex with reasoning]
- **Data Assessment:** [Available/Adaptable/Problematic with details]
- **Resource Requirements:** [Computational needs and constraints]
- **Reproducibility Score:** [High/Medium/Low with justification]
- **Recommendation:** [Go/No-Go with confidence level]
- **Implementation Roadmap:** [If Go: high-level approach and timeline]

**Success Criteria:**
- Clear go/no-go recommendation with reasoning
- Realistic timeline estimate (days/weeks)
- Identification of major risks and mitigation strategies
```

### Follow-up AI Prompts

**Deep Algorithm Analysis:**
```markdown
**Context:** Based on your initial analysis, dive deeper into the algorithm implementation.

**Specific Focus:** 
- Break down the algorithm into implementable components
- Identify the most challenging implementation aspects
- Suggest simplifications that preserve core concepts
- Recommend validation approaches

**Output:** Step-by-step implementation strategy with complexity assessment for each component.
```

**Risk Assessment:**
```markdown
**Context:** Analyze potential failure modes and mitigation strategies.

**Focus Areas:**
- Technical risks (algorithm complexity, data issues, performance)
- Resource risks (computational requirements, time constraints)
- Knowledge risks (domain expertise gaps, unclear specifications)

**Output:** Risk matrix with likelihood, impact, and mitigation strategies for top 5 risks.
```

## Generated Tasks (AI-Created)
Based on the AI analysis, expect these types of tasks:

1. **Paper Deep-Dive Analysis**
   - Extract mathematical formulations and pseudocode
   - Identify key assumptions and limitations
   - Map algorithm to existing implementations (if any)

2. **Data Landscape Assessment**
   - Research dataset availability and licensing
   - Assess data preprocessing requirements
   - Plan synthetic data generation if needed

3. **Technical Dependency Mapping**
   - Identify required libraries and frameworks
   - Check version compatibility and installation complexity
   - Plan development environment setup

4. **Computational Feasibility Study**
   - Estimate memory and processing requirements
   - Assess scalability needs for POC vs. full implementation
   - Plan cloud resources if local resources insufficient

## Deliverables

### 1. Feasibility Assessment Report
```yaml
## Paper Information
Title: [Full paper title]
Authors: [Author list]
Venue: [Conference/Journal, Year]
Domain: [Research area]
Paper URL: [Link to paper]

## Algorithm Analysis
Core Contribution: [Main algorithmic innovation]
Algorithm Type: [Classification: optimization, generative, discriminative, etc.]
Mathematical Complexity: [Analysis of core equations and computational steps]
Implementation Clarity: [How well the algorithm is described]

## Data Requirements
Primary Dataset: [Main dataset used in paper]
Dataset Size: [Volume and dimensionality]
Availability: [Public/Private/Synthetic possible]
Preprocessing Needs: [Required data transformations]
Alternative Datasets: [Substitutes if primary unavailable]

## Computational Assessment
Memory Requirements: [Estimated RAM needs]
Processing Requirements: [CPU/GPU needs, training time]
Scalability: [How requirements scale with data/model size]
Development Environment: [Recommended setup]

## Dependencies
Core Libraries: [Essential frameworks and versions]
Optional Tools: [Nice-to-have additions]
Installation Complexity: [Setup difficulty assessment]
Compatibility Issues: [Known conflicts or challenges]

## Go/No-Go Decision
Recommendation: [GO / NO-GO / CONDITIONAL]
Confidence Level: [High / Medium / Low]
Reasoning: [Key factors influencing decision]
Major Risks: [Top 3 implementation risks]
Success Probability: [Estimated likelihood of successful POC]

## Implementation Strategy (if GO)
Approach: [High-level implementation plan]
Timeline: [Realistic duration estimate]
Milestones: [Key checkpoints and deliverables]
Success Criteria: [How to measure POC success]
Minimum Viable Implementation: [Simplest version that proves concept]
```

### 2. Technical Roadmap
```yaml
## Implementation Phases
Phase 1: [Environment setup and data pipeline]
Phase 2: [Core algorithm implementation]
Phase 3: [Integration and validation]
Phase 4: [Optimization and documentation]

## Component Breakdown
Data Pipeline: [Input processing and feature extraction]
Core Algorithm: [Main algorithmic components]
Validation Framework: [Testing and evaluation approach]
Visualization: [Results presentation and interpretation]

## Technology Stack
Language: [Primary programming language]
ML Framework: [TensorFlow, PyTorch, Scikit-learn, etc.]
Data Processing: [Pandas, NumPy, specialized tools]
Visualization: [Matplotlib, Plotly, custom tools]
Development: [IDE, version control, environment management]
```

## Validation Checkpoints

### AI Self-Assessment Checklist
- [ ] Algorithm description is sufficiently detailed for implementation
- [ ] Data requirements are realistic and achievable
- [ ] Computational complexity is within reasonable bounds
- [ ] Dependencies are available and compatible
- [ ] Timeline estimate is realistic based on complexity assessment
- [ ] Risk assessment covers major failure modes
- [ ] Success criteria are clear and measurable

### Human Review Checklist
- [ ] **Domain Expertise:** AI analysis aligns with your understanding of the field
- [ ] **Resource Reality:** Computational and time estimates match your constraints
- [ ] **Risk Tolerance:** Identified risks are acceptable for your learning goals
- [ ] **Value Alignment:** POC will provide sufficient learning/demonstration value
- [ ] **Next Steps:** Clear path forward if decision is GO

### Decision Gates

**GREEN LIGHT (Proceed immediately):**
- All feasibility criteria met with high confidence
- Clear implementation path with manageable complexity
- Available data and computational resources
- High learning value and demonstration potential

**YELLOW LIGHT (Proceed with modifications):**
- Some feasibility concerns but acceptable with adaptations
- May require simplified implementation or alternative datasets
- Moderate complexity but within skill/time budget
- Good learning opportunity despite challenges

**RED LIGHT (Do not proceed):**
- Major feasibility concerns that cannot be resolved
- Requires resources or expertise significantly beyond current capacity
- Unclear or incomplete algorithm description
- Low probability of successful POC within time constraints

## Exit Criteria
- [ ] Clear go/no-go decision documented with reasoning
- [ ] If GO: technical roadmap and timeline established
- [ ] Major risks identified with mitigation strategies
- [ ] Success criteria defined for eventual POC
- [ ] Next stage (Investigation) is properly set up

---

# Stage 2: Spike/Investigation
**Duration:** 2-8 hours  
**AI Persona:** Technical Investigator  
**Primary Goal:** Design system architecture and validate critical assumptions through targeted experiments

## Entry Criteria
- [ ] Stage 1 completed with GO decision
- [ ] Feasibility assessment and technical roadmap available
- [ ] Development environment set up
- [ ] Key risks and unknowns identified

## High-Level Objective
Conduct time-boxed technical investigation to design the system architecture, validate critical assumptions, and de-risk the implementation before writing the full specification.

## AI Collaboration Pattern: Medium-Autonomy Research

### Primary AI Prompt
```markdown
**Role:** You are a technical architect specializing in [DOMAIN] with deep implementation experience.

**Context:** 
- Paper: [PAPER_TITLE]
- Go decision made based on: [FEASIBILITY_SUMMARY]
- Key risks identified: [TOP_3_RISKS]
- Available resources: [COMPUTE_RESOURCES, TIME_BUDGET]

**Investigation Goals:**
1. Design system architecture that balances simplicity with functionality
2. Validate critical technical assumptions through targeted experiments
3. Create implementation plan that minimizes identified risks
4. Establish success metrics and validation approaches

**Specific Investigation Areas:**
- **Architecture Design:** How should components be structured and integrated?
- **Data Pipeline:** What is the end-to-end data flow and processing approach?
- **Algorithm Implementation:** What are the critical implementation details and potential pitfalls?
- **Validation Strategy:** How will we verify the implementation works correctly?

**Output Requirements:**
- System architecture diagram (text/ASCII representation)
- Component interaction specification
- Data flow design with sample transformations
- Risk mitigation strategies for top technical challenges
- Success metrics and validation approach
- Recommendation for next steps

**Investigation Method:**
- Research similar implementations and approaches
- Design experiments to validate critical assumptions
- Create minimal prototypes for high-risk components
- Document findings and recommendations
```

### Targeted Investigation Prompts

**Architecture Deep-Dive:**
```markdown
**Focus:** Design the overall system architecture for the POC.

**Requirements:**
- Modular design with clear component boundaries
- Separation of concerns (data, algorithm, validation, visualization)
- Extensible structure for future improvements
- Simple interfaces that enable testing and debugging

**Deliverable:** 
- Architecture diagram showing major components
- Interface specifications between components
- Data model design
- Error handling and logging strategy
```

**Critical Path Analysis:**
```markdown
**Focus:** Identify the implementation path that minimizes risk and maximizes learning.

**Analysis Areas:**
- Which components are most critical to implement first?
- What are the highest-risk implementation challenges?
- Where can we use existing libraries vs. custom implementation?
- What experiments can validate our approach early?

**Deliverable:**
- Implementation order with rationale
- Risk assessment for each major component
- Validation experiments for critical assumptions
- Library selection and justification
```

**Data Pipeline Design:**
```markdown
**Focus:** Design the end-to-end data processing pipeline.

**Requirements:**
- Input data ingestion and validation
- Preprocessing and feature extraction
- Algorithm input preparation
- Output processing and visualization
- Error handling for data quality issues

**Deliverable:**
- Data flow diagram from raw input to final output
- Data transformation specifications
- Quality validation checkpoints
- Sample data processing examples
```

## Generated Tasks (AI-Created)

1. **System Architecture Design**
   - Create component diagram and interaction specification
   - Define interfaces and data contracts between components
   - Plan error handling and logging strategy

2. **Data Pipeline Prototyping**
   - Implement basic data loading and preprocessing
   - Validate data quality and format assumptions
   - Create sample data transformations

3. **Algorithm Core Prototyping**
   - Implement simplified version of core algorithm
   - Validate mathematical implementations with known examples
   - Test performance characteristics with sample data

4. **Integration Experiment**
   - Build end-to-end minimal pipeline
   - Test component interactions and data flow
   - Identify integration challenges and solutions

5. **Validation Framework Design**
   - Define success metrics and evaluation criteria
   - Plan testing strategy for algorithm correctness
   - Design visualization and interpretation approaches

## Deliverables

### 1. System Architecture Specification
```yaml
## Architecture Overview
Design Philosophy: [Modular/Monolithic, rationale]
Component Structure: [Major system components]
Technology Choices: [Languages, frameworks, tools]
Integration Pattern: [How components communicate]

## Component Specifications
### Data Pipeline
Responsibility: [Data ingestion, processing, validation]
Inputs: [Raw data sources and formats]
Outputs: [Processed data for algorithm consumption]
Key Functions: [Critical processing steps]
Dependencies: [External libraries and services]

### Core Algorithm
Responsibility: [Main algorithmic processing]
Inputs: [Processed data specification]
Outputs: [Algorithm results and metadata]
Key Functions: [Core computational components]
Performance Targets: [Speed, memory, accuracy goals]

### Validation Module
Responsibility: [Results validation and evaluation]
Inputs: [Algorithm outputs and ground truth]
Outputs: [Evaluation metrics and visualizations]
Key Functions: [Validation tests and quality checks]

### Visualization Layer
Responsibility: [Results presentation and interpretation]
Inputs: [Processed results and evaluation metrics]
Outputs: [Charts, graphs, reports]
Key Functions: [Visualization generation and formatting]

## Data Flow Design
Pipeline Stages: [Sequential processing steps]
Data Transformations: [Key data modifications]
Quality Gates: [Validation checkpoints]
Error Handling: [Failure recovery strategies]

## Interface Specifications
Component APIs: [Function signatures and data contracts]
Data Formats: [Standard formats for inter-component communication]
Error Protocols: [How components handle and report errors]
Configuration: [Parameterization and customization approaches]
```

### 2. Risk Mitigation Plan
```yaml
## Technical Risk Assessment
### High Priority Risks
Risk 1: [Description]
  Likelihood: [High/Medium/Low]
  Impact: [High/Medium/Low]
  Mitigation: [Specific actions to reduce risk]
  Validation: [How to test if mitigation works]

Risk 2: [Description]
  Likelihood: [High/Medium/Low]
  Impact: [High/Medium/Low]
  Mitigation: [Specific actions to reduce risk]
  Validation: [How to test if mitigation works]

Risk 3: [Description]
  Likelihood: [High/Medium/Low]
  Impact: [High/Medium/Low]
  Mitigation: [Specific actions to reduce risk]
  Validation: [How to test if mitigation works]

## Implementation Strategy
Critical Path: [Most important components to implement first]
Validation Points: [Where to test assumptions and validate approach]
Fallback Options: [Alternative approaches if primary plan fails]
Success Criteria: [How to measure progress and eventual success]
```

### 3. Prototype Experiments Results
```yaml
## Experiment 1: [Data Pipeline Validation]
Objective: [What was being tested]
Method: [How the experiment was conducted]
Results: [What was discovered]
Implications: [Impact on implementation approach]
Next Steps: [Follow-up actions required]

## Experiment 2: [Algorithm Core Validation]
Objective: [What was being tested]
Method: [How the experiment was conducted]
Results: [What was discovered]
Implications: [Impact on implementation approach]
Next Steps: [Follow-up actions required]

## Experiment 3: [Integration Testing]
Objective: [What was being tested]
Method: [How the experiment was conducted]
Results: [What was discovered]
Implications: [Impact on implementation approach]
Next Steps: [Follow-up actions required]
```

## Validation Checkpoints

### AI Self-Assessment Checklist
- [ ] Architecture design is modular and testable
- [ ] Component interfaces are clearly specified
- [ ] Data flow design handles edge cases and errors
- [ ] Risk mitigation strategies are specific and actionable
- [ ] Success criteria are measurable and realistic
- [ ] Implementation path minimizes complexity and risk

### Human Review Checklist
- [ ] **Architecture Feasibility:** Design is implementable within time/skill constraints
- [ ] **Risk Coverage:** Major technical risks are identified and addressable
- [ ] **Implementation Clarity:** Clear path forward for building each component
- [ ] **Success Definition:** Metrics align with learning and demonstration goals
- [ ] **Resource Planning:** Computational and time requirements are realistic

### Experiment Validation
- [ ] **Data Pipeline:** Can successfully process sample data
- [ ] **Algorithm Core:** Mathematical implementation matches expected behavior
- [ ] **Integration:** Components can communicate and exchange data
- [ ] **Performance:** Initial performance meets basic requirements
- [ ] **Error Handling:** System gracefully handles common failure modes

## Exit Criteria
- [ ] System architecture documented and validated through prototypes
- [ ] Major technical risks identified with specific mitigation strategies
- [ ] Implementation approach designed to minimize complexity and maximize success
- [ ] Success criteria and validation methods established
- [ ] Ready to proceed to detailed specification stage

---

# Stage 3: Specification
**Duration:** 1-2 hours  
**AI Persona:** System Architect  
**Primary Goal:** Create detailed implementation specification that enables systematic ticket generation and execution

## Entry Criteria
- [ ] Stage 2 completed with validated architecture
- [ ] System design and component specifications available
- [ ] Risk mitigation strategies defined
- [ ] Success criteria established

## High-Level Objective
Transform the architecture design and investigation results into a concrete implementation specification that can be broken down into specific, executable tasks.

## AI Collaboration Pattern: High-Autonomy Documentation

### Primary AI Prompt
```markdown
**Role:** You are a senior technical architect creating implementation specifications for development teams.

**Context:**
- Paper: [PAPER_TITLE]
- System Architecture: [ARCHITECTURE_SUMMARY from Stage 2]
- Key Components: [COMPONENT_LIST]
- Success Criteria: [DEFINED_METRICS]
- Timeline: [AVAILABLE_TIME_BUDGET]

**Task:** Create a comprehensive but lightweight implementation specification that enables systematic development.

**Specification Requirements:**
1. **Implementation Overview:** High-level approach and key design decisions
2. **Component Specifications:** Detailed requirements for each major component
3. **Interface Definitions:** APIs, data formats, and integration points
4. **Implementation Plan:** Logical sequence and dependencies
5. **Testing Strategy:** Validation approach and success metrics
6. **Quality Standards:** Code quality, documentation, and performance requirements

**Output Format:**
- Clear, actionable specifications for each component
- Specific implementation requirements and constraints
- Testable acceptance criteria for all major features
- Integration and deployment considerations
- Risk mitigation embedded in the implementation approach

**Quality Criteria:**
- Specific enough to generate concrete development tasks
- Complete enough to avoid major rework during implementation
- Flexible enough to adapt to discoveries during development
- Clear enough for someone else to implement from the specification
```

### Component Deep-Dive Prompts

**Data Pipeline Specification:**
```markdown
**Focus:** Create detailed specification for the data processing pipeline.

**Requirements:**
- Input data formats and validation requirements
- Preprocessing steps with specific transformations
- Output data schema and quality requirements
- Error handling for data quality issues
- Performance requirements and optimization opportunities

**Deliverable:** Complete data pipeline specification with implementation details, validation requirements, and performance criteria.
```

**Algorithm Implementation Specification:**
```markdown
**Focus:** Specify the core algorithm implementation in implementable detail.

**Requirements:**
- Mathematical formulations translated to computational steps
- Data structures and algorithmic approaches
- Performance optimization strategies
- Validation and testing approaches
- Integration with data pipeline and output systems

**Deliverable:** Algorithmic specification with pseudocode, data structures, optimization considerations, and validation requirements.
```

**Integration & Deployment Specification:**
```markdown
**Focus:** Specify how components integrate and how the system will be deployed and validated.

**Requirements:**
- Component integration patterns and interfaces
- End-to-end system validation approach
- Deployment and execution requirements
- Monitoring and debugging capabilities
- Documentation and demonstration requirements

**Deliverable:** Integration specification with deployment plan, validation strategy, and operational requirements.
```

## Generated Tasks (AI-Created)

1. **Component Specification Creation**
   - Write detailed specifications for each major component
   - Define interfaces, inputs, outputs, and behavior requirements
   - Specify quality and performance criteria

2. **Implementation Planning**
   - Define development sequence and component dependencies
   - Plan integration points and testing strategies
   - Identify reusable components and libraries

3. **Success Criteria Definition**
   - Translate high-level goals into measurable outcomes
   - Define acceptance criteria for each component
   - Plan validation and demonstration approaches

4. **Risk Integration**
   - Embed risk mitigation strategies into implementation approach
   - Plan contingency approaches for high-risk components
   - Define early validation points for critical assumptions

## Deliverables

### 1. Implementation Specification Document
```yaml
## Project Overview
Objective: [Specific POC goals and success criteria]
Scope: [What will and will not be implemented]
Timeline: [Development phases and milestones]
Success Metrics: [Measurable outcomes and acceptance criteria]

## System Architecture
Design Overview: [High-level system structure and component relationships]
Technology Stack: [Programming languages, frameworks, libraries, tools]
Development Environment: [Setup requirements and configuration]
Deployment Target: [Where and how the system will run]

## Component Specifications

### Data Pipeline Component
Purpose: [Component responsibility and objectives]
Inputs: 
  - Format: [Data format specifications]
  - Sources: [Where data comes from]
  - Validation: [Data quality requirements]
Processing:
  - Transformations: [Specific data processing steps]
  - Quality Checks: [Validation and error detection]
  - Performance: [Speed and memory requirements]
Outputs:
  - Format: [Output data specification]
  - Quality: [Output validation requirements]
  - Interface: [How other components access the data]
Implementation Notes:
  - Libraries: [Specific tools and frameworks to use]
  - Algorithms: [Processing approaches and optimizations]
  - Error Handling: [How to handle and report failures]

### Core Algorithm Component
Purpose: [Component responsibility and objectives]
Inputs:
  - Data Requirements: [Input data format and validation]
  - Parameters: [Configurable algorithm parameters]
  - Dependencies: [Required external resources]
Processing:
  - Algorithm Steps: [Detailed computational process]
  - Data Structures: [Internal representations and storage]
  - Optimization: [Performance enhancement strategies]
Outputs:
  - Results Format: [Algorithm output specification]
  - Metadata: [Additional information and diagnostics]
  - Quality Metrics: [Self-assessment and validation data]
Implementation Notes:
  - Mathematical Details: [Key equations and computational approaches]
  - Libraries: [Specific implementations and tools]
  - Testing: [Validation approaches and test cases]

### Validation Component
Purpose: [Component responsibility and objectives]
Inputs:
  - Results: [Algorithm outputs to validate]
  - Ground Truth: [Reference data for comparison]
  - Metrics: [Evaluation criteria and methods]
Processing:
  - Evaluation: [Statistical analysis and comparison methods]
  - Visualization: [Charts, graphs, and presentation formats]
  - Reporting: [Summary generation and interpretation]
Outputs:
  - Metrics: [Quantitative evaluation results]
  - Visualizations: [Charts and graphs for interpretation]
  - Reports: [Summary documents and recommendations]
Implementation Notes:
  - Statistical Methods: [Evaluation approaches and significance testing]
  - Visualization Tools: [Libraries and presentation formats]
  - Interpretation: [How to understand and present results]

## Interface Specifications
### Component APIs
Data Pipeline â†’ Algorithm:
  - Function: [Interface specification]
  - Parameters: [Input parameters and types]
  - Returns: [Output format and structure]
  - Errors: [Exception handling and error codes]

Algorithm â†’ Validation:
  - Function: [Interface specification]
  - Parameters: [Input parameters and types]
  - Returns: [Output format and structure]
  - Errors: [Exception handling and error codes]

### Data Formats
Internal Data Format:
  - Structure: [Data organization and schema]
  - Types: [Field types and constraints]
  - Validation: [Quality requirements and checks]

Configuration Format:
  - Parameters: [Configurable settings and options]
  - Validation: [Parameter validation and defaults]
  - Documentation: [Parameter descriptions and usage]

## Implementation Plan
### Development Phases
Phase 1: Foundation (Timeline: [Duration])
  - Environment setup and configuration
  - Basic project structure and documentation
  - Initial data pipeline implementation
  - Success Criteria: [Specific deliverables and validation]

Phase 2: Core Algorithm (Timeline: [Duration])
  - Algorithm implementation and testing
  - Integration with data pipeline
  - Basic validation and debugging
  - Success Criteria: [Specific deliverables and validation]

Phase 3: Integration & Validation (Timeline: [Duration])
  - End-to-end system integration
  - Comprehensive testing and validation
  - Performance optimization and debugging
  - Success Criteria: [Specific deliverables and validation]

Phase 4: Documentation & Demo (Timeline: [Duration])
  - Documentation completion and review
  - Demonstration preparation and testing
  - Final validation and quality assurance
  - Success Criteria: [Specific deliverables and validation]

### Dependencies & Risks
Critical Dependencies:
  - [Dependency 1]: [Description and mitigation if unavailable]
  - [Dependency 2]: [Description and mitigation if unavailable]

High-Risk Components:
  - [Component]: [Risk description and mitigation strategy]
  - [Component]: [Risk description and mitigation strategy]

## Quality Standards
### Code Quality
Standards: [Coding style, naming conventions, documentation requirements]
Testing: [Unit test coverage, integration testing, validation requirements]
Performance: [Speed requirements, memory constraints, optimization targets]
Maintainability: [Code organization, documentation, extensibility requirements]

### Documentation Requirements
Code Documentation: [Inline comments, function documentation, API specifications]
User Documentation: [Setup instructions, usage examples, troubleshooting guides]
Technical Documentation: [Architecture overview, design decisions, implementation notes]
Validation Documentation: [Test results, performance metrics, validation reports]

## Success Criteria & Validation
### Functional Requirements
Core Functionality: [Primary algorithm implementation and validation]
Data Processing: [Input handling, processing accuracy, output generation]
Integration: [Component interaction, end-to-end functionality]
Performance: [Speed, memory usage, scalability within POC scope]

### Quality Requirements
Correctness: [Algorithm accuracy, result validation, error handling]
Robustness: [Input validation, error recovery, edge case handling]
Usability: [Setup simplicity, execution clarity, result interpretation]
Maintainability: [Code clarity, documentation completeness, extensibility]

### Demonstration Requirements
Working Demo: [End-to-end execution with sample data]
Result Validation: [Comparison with paper results or expected outcomes]
Documentation: [Complete setup, usage, and interpretation instructions]
Knowledge Transfer: [Clear explanation of implementation approach and learnings]
```

## Validation Checkpoints

### AI Self-Assessment Checklist
- [ ] All major components have detailed, implementable specifications
- [ ] Interfaces between components are clearly defined
- [ ] Implementation plan is logical and accounts for dependencies
- [ ] Success criteria are specific, measurable, and realistic
- [ ] Quality standards are appropriate for POC scope
- [ ] Risk mitigation is embedded in implementation approach

### Human Review Checklist
- [ ] **Implementability:** Specifications provide sufficient detail for development
- [ ] **Completeness:** All necessary components and integrations are specified
- [ ] **Feasibility:** Implementation plan is realistic given time and resource constraints
- [ ] **Quality:** Standards are appropriate for learning and demonstration goals
- [ ] **Clarity:** Specifications are clear enough for systematic task generation

### Specification Quality Gates
- [ ] **Component Completeness:** All major components specified with inputs, processing, outputs
- [ ] **Interface Clarity:** Component interactions clearly defined and testable
- [ ] **Implementation Detail:** Sufficient technical detail for code generation
- [ ] **Validation Plan:** Clear approach for testing and validating each component
- [ ] **Success Definition:** Measurable criteria for successful POC completion

## Exit Criteria
- [ ] Complete implementation specification documented and validated
- [ ] Component interfaces and integration points clearly defined
- [ ] Implementation plan with realistic timeline and milestones
- [ ] Success criteria and validation approach established
- [ ] Quality standards appropriate for POC scope
- [ ] Ready for systematic task breakdown and ticket generation

---

# Stage 4: Ticket Planning
**Duration:** 30-60 minutes  
**AI Persona:** Project Manager  
**Primary Goal:** Transform implementation specification into structured, executable tasks with clear acceptance criteria

## Entry Criteria
- [ ] Stage 3 completed with detailed implementation specification
- [ ] Component specifications and interfaces defined
- [ ] Implementation plan with phases and milestones
- [ ] Success criteria and quality standards established

## High-Level Objective
Break down the implementation specification into manageable, trackable tickets that can be executed systematically with clear acceptance criteria and AI collaboration guidance.

## AI Collaboration Pattern: High-Autonomy Task Generation

### Primary AI Prompt
```markdown
**Role:** You are an experienced project manager specializing in technical project breakdown and task management.

**Context:**
- Project: [PAPER_TITLE] POC Implementation
- Implementation Spec: [SPECIFICATION_SUMMARY]
- Timeline: [TOTAL_AVAILABLE_TIME]
- Development Phases: [PHASE_BREAKDOWN]
- Success Criteria: [PROJECT_SUCCESS_METRICS]

**Task:** Generate a comprehensive set of structured tickets that cover all aspects of the implementation specification.

**Ticket Generation Requirements:**
1. **Complete Coverage:** Every component and requirement from the spec should map to tickets
2. **Right-Sized:** Each ticket should be 2-6 hours of work (half-day to full-day maximum)
3. **Clear Dependencies:** Tickets should have explicit prerequisites and sequencing
4. **Testable:** Each ticket should have specific, verifiable acceptance criteria
5. **AI-Collaborative:** Include specific AI collaboration guidance for each ticket

**Ticket Categories to Generate:**
- **Setup & Environment:** Project infrastructure and development environment
- **Data Pipeline:** Data processing, validation, and transformation
- **Core Algorithm:** Main algorithmic implementation and optimization
- **Integration:** Component connectivity and end-to-end system
- **Validation:** Testing, evaluation, and quality assurance
- **Documentation:** Code documentation, user guides, and demonstration materials

**Output Format for Each Ticket:**
```yaml
TICKET-ID: [WORKFLOW-STAGE-SEQUENCE]
Title: [Action-oriented description]
Category: [Setup/Data/Algorithm/Integration/Validation/Documentation]
Priority: [High/Medium/Low]
Estimated Duration: [Hours]
Dependencies: [List of prerequisite tickets]

Description: |
  [Detailed description of what needs to be accomplished]

Acceptance Criteria:
  - [ ] [Specific, testable outcome 1]
  - [ ] [Specific, testable outcome 2]
  - [ ] [Specific, testable outcome 3]

AI Collaboration Instructions: |
  [Specific guidance for AI assistance including:]
  - Primary AI role (code generator, reviewer, advisor)
  - Specific prompts or collaboration patterns to use
  - Expected AI contribution level (high/medium/low autonomy)
  - Quality validation approaches

Validation Checkpoints:
  - [Checkpoint 1]: [What to validate and how]
  - [Checkpoint 2]: [What to validate and how]
  - [Final Review]: [Human validation requirements]

Definition of Done:
  - [ ] All acceptance criteria met
  - [ ] Code reviewed and tested
  - [ ] Documentation updated
  - [ ] Integration verified
```

**Quality Requirements:**
- No ticket should take more than 1 day of work
- Every ticket should have 3-5 testable acceptance criteria
- Dependencies should form a logical implementation sequence
- AI collaboration guidance should be specific and actionable
```

### Ticket Refinement Prompts

**Dependency Analysis:**
```markdown
**Context:** Review the generated tickets for logical sequencing and dependencies.

**Analysis Requirements:**
- Identify the critical path through all tickets
- Ensure no circular dependencies exist
- Validate that dependencies are realistic and necessary
- Suggest parallel work opportunities where possible

**Output:** 
- Dependency graph or sequence diagram
- Critical path identification
- Parallel execution opportunities
- Dependency conflict resolution
```

**Effort Validation:**
```markdown
**Context:** Validate that ticket estimates are realistic and well-distributed.

**Validation Criteria:**
- No single ticket should exceed 8 hours of work
- Tickets should be balanced across different types of tasks
- High-risk or complex tasks should be broken down further
- Estimates should account for learning curve and debugging time

**Output:**
- Effort redistribution recommendations
- Risk assessment for high-estimate tickets
- Suggestions for ticket splitting if needed
- Overall timeline validation
```

## Generated Ticket Categories

### Setup & Environment Tickets
- Development environment configuration
- Project structure and repository setup
- Dependency installation and testing
- Data acquisition and initial validation

### Data Pipeline Tickets
- Raw data ingestion and validation
- Data preprocessing and transformation
- Feature extraction and preparation
- Data quality assurance and error handling

### Core Algorithm Tickets
- Mathematical implementation and validation
- Algorithm optimization and performance tuning
- Parameter configuration and tuning
- Integration with data pipeline

### Integration Tickets
- Component interface implementation
- End-to-end pipeline integration
- System configuration and parameter management
- Error handling and logging integration

### Validation Tickets
- Unit testing for individual components
- Integration testing for system workflows
- Performance benchmarking and optimization
- Result validation and accuracy assessment

### Documentation Tickets
- Code documentation and commenting
- User guide and setup instructions
- Technical documentation and architecture notes
- Demonstration preparation and presentation materials

## Example Ticket Set

### RESEARCH-PLANNING-001: Project Setup and Environment Configuration
```yaml
TICKET-ID: RESEARCH-PLANNING-001
Title: Set up development environment and project structure
Category: Setup
Priority: High
Estimated Duration: 2-3 hours
Dependencies: []

Description: |
  Create the foundational project structure, configure development environment,
  and establish basic project infrastructure including version control, 
  dependency management, and initial documentation.

Acceptance Criteria:
  - [ ] Git repository initialized with appropriate .gitignore
  - [ ] Virtual environment created with all required dependencies
  - [ ] Project directory structure matches specification
  - [ ] Basic README with setup instructions created
  - [ ] All dependencies install without conflicts

AI Collaboration Instructions: |
  Role: Setup Assistant
  - Use AI to generate appropriate .gitignore for the technology stack
  - Get AI help with dependency compatibility checking
  - Request AI to review project structure for best practices
  - Ask AI to draft initial README template
  
  Collaboration Level: Medium autonomy
  Primary Tools: ChatGPT for configuration advice, GitHub Copilot for file generation

Validation Checkpoints:
  - Environment Test: All dependencies import successfully
  - Structure Validation: Project matches architectural specification
  - Final Review: Human verification of setup completeness

Definition of Done:
  - [ ] Development environment fully functional
  - [ ] Project structure created and documented
  - [ ] Dependencies installed and tested
  - [ ] Initial documentation committed to repository
```

### RESEARCH-DATA-001: Implement Data Loading and Validation Pipeline
```yaml
TICKET-ID: RESEARCH-DATA-001
Title: Implement data loading, validation, and initial preprocessing
Category: Data
Priority: High
Estimated Duration: 4-6 hours
Dependencies: [RESEARCH-PLANNING-001]

Description: |
  Create the data pipeline component that handles raw data ingestion,
  validates data quality and format, and performs initial preprocessing
  steps as specified in the implementation specification.

Acceptance Criteria:
  - [ ] Data loading function handles specified input formats
  - [ ] Data validation catches and reports format/quality issues
  - [ ] Initial preprocessing transforms data to expected format
  - [ ] Pipeline handles missing data and edge cases gracefully
  - [ ] Unit tests cover normal and error cases

AI Collaboration Instructions: |
  Role: Code Generator and Validator
  - Generate data loading boilerplate for specified formats
  - Create comprehensive data validation logic
  - Implement preprocessing transformations step-by-step
  - Generate unit tests for all major functions
  
  Collaboration Level: High autonomy
  Primary Tools: GitHub Copilot for implementation, ChatGPT for debugging
  
  Specific Prompts:
  - "Generate pandas data loading function for [FORMAT] with error handling"
  - "Create data validation schema for [DATA_REQUIREMENTS]"
  - "Write unit tests for data pipeline with edge cases"

Validation Checkpoints:
  - Data Loading Test: Successfully loads sample data files
  - Validation Test: Correctly identifies and handles data quality issues
  - Processing Test: Transformations produce expected output format
  - Final Review: Human verification of pipeline correctness

Definition of Done:
  - [ ] All acceptance criteria met
  - [ ] Unit tests pass with >90% coverage
  - [ ] Code documented with docstrings and comments
  - [ ] Integration interface ready for next component
```

### RESEARCH-ALGORITHM-001: Implement Core Algorithm Component
```yaml
TICKET-ID: RESEARCH-ALGORITHM-001
Title: Implement main algorithm as specified in paper
Category: Algorithm
Priority: High
Estimated Duration: 6-8 hours
Dependencies: [RESEARCH-DATA-001]

Description: |
  Implement the core algorithm described in the research paper, including
  all mathematical computations, data transformations, and optimization
  steps as detailed in the implementation specification.

Acceptance Criteria:
  - [ ] Algorithm implements all mathematical formulations from paper
  - [ ] Input/output interfaces match specification requirements
  - [ ] Algorithm handles edge cases and numerical stability issues
  - [ ] Performance meets basic requirements for POC scope
  - [ ] Implementation includes parameter configuration options

AI Collaboration Instructions: |
  Role: Mathematical Implementation Assistant
  - Translate mathematical notation to executable code
  - Generate numerical computation implementations
  - Create parameter configuration and validation logic
  - Implement optimization strategies for performance
  
  Collaboration Level: High autonomy with human mathematical validation
  Primary Tools: GitHub Copilot for code generation, ChatGPT for mathematical interpretation
  
  Specific Prompts:
  - "Implement [EQUATION] from the paper in [LANGUAGE] with numerical stability"
  - "Generate parameter class for algorithm configuration with validation"
  - "Optimize [ALGORITHM_STEP] for performance while maintaining accuracy"

Validation Checkpoints:
  - Mathematical Correctness: Verify computations match paper formulations
  - Numerical Testing: Test with known inputs and expected outputs
  - Performance Check: Verify speed/memory requirements are reasonable
  - Final Review: Human validation of algorithmic correctness

Definition of Done:
  - [ ] All acceptance criteria met
  - [ ] Mathematical implementation validated against paper
  - [ ] Performance benchmarks within acceptable range
  - [ ] Code thoroughly documented with algorithm explanation
```

### RESEARCH-INTEGRATION-001: Integrate Components and Test End-to-End Pipeline
```yaml
TICKET-ID: RESEARCH-INTEGRATION-001
Title: Integrate all components into working end-to-end system
Category: Integration
Priority: High
Estimated Duration: 3-4 hours
Dependencies: [RESEARCH-DATA-001, RESEARCH-ALGORITHM-001, RESEARCH-VALIDATION-001]

Description: |
  Connect all implemented components into a cohesive system that can
  execute the complete workflow from raw data input to final results,
  including error handling and logging throughout the pipeline.

Acceptance Criteria:
  - [ ] All components integrate without interface conflicts
  - [ ] End-to-end execution completes successfully with sample data
  - [ ] Error handling works correctly across component boundaries
  - [ ] Logging provides sufficient detail for debugging and monitoring
  - [ ] System configuration allows for parameter adjustment

AI Collaboration Instructions: |
  Role: Integration Specialist
  - Generate integration code connecting component interfaces
  - Create configuration management for system parameters
  - Implement comprehensive error handling and logging
  - Generate integration tests for end-to-end workflows
  
  Collaboration Level: Medium autonomy with human architectural validation
  Primary Tools: GitHub Copilot for integration code, ChatGPT for architecture advice
  
  Specific Prompts:
  - "Create main execution pipeline connecting [COMPONENTS]"
  - "Generate configuration system for [PARAMETERS] with validation"
  - "Implement error handling strategy for [INTEGRATION_POINTS]"

Validation Checkpoints:
  - Integration Test: All components connect and communicate properly
  - End-to-End Test: Complete workflow executes successfully
  - Error Handling Test: System handles failures gracefully
  - Final Review: Human validation of integration architecture

Definition of Done:
  - [ ] All acceptance criteria met
  - [ ] End-to-end integration tests pass
  - [ ] Error handling verified across all components
  - [ ] System ready for validation and demonstration
```

## Deliverables

### 1. Complete Ticket List
```yaml
## Ticket Summary
Total Tickets: [NUMBER]
Estimated Total Duration: [HOURS]
Critical Path Duration: [HOURS]
Parallel Execution Opportunities: [DESCRIPTION]

## Tickets by Category
Setup & Environment: [COUNT] tickets, [HOURS] total
Data Pipeline: [COUNT] tickets, [HOURS] total
Core Algorithm: [COUNT] tickets, [HOURS] total
Integration: [COUNT] tickets, [HOURS] total
Validation: [COUNT] tickets, [HOURS] total
Documentation: [COUNT] tickets, [HOURS] total

## Execution Plan
Phase 1 (Foundation): [TICKET_LIST] - [DURATION]
Phase 2 (Core Implementation): [TICKET_LIST] - [DURATION]
Phase 3 (Integration & Testing): [TICKET_LIST] - [DURATION]
Phase 4 (Validation & Documentation): [TICKET_LIST] - [DURATION]

## Risk Assessment
High-Risk Tickets: [LIST with mitigation strategies]
Critical Path Bottlenecks: [IDENTIFICATION and alternatives]
Resource Requirements: [COMPUTATIONAL/TIME needs per phase]
```

### 2. Dependency Graph
```yaml
## Execution Sequence
### Foundation Layer (Can execute in parallel)
- RESEARCH-PLANNING-001: Project Setup
- RESEARCH-PLANNING-002: Data Acquisition

### Data Layer (Requires foundation)
- RESEARCH-DATA-001: Data Pipeline (depends on PLANNING-001)
- RESEARCH-DATA-002: Data Validation (depends on DATA-001)

### Algorithm Layer (Requires data)
- RESEARCH-ALGORITHM-001: Core Implementation (depends on DATA-001)
- RESEARCH-ALGORITHM-002: Algorithm Testing (depends on ALGORITHM-001)

### Integration Layer (Requires all components)
- RESEARCH-INTEGRATION-001: Component Integration (depends on all previous layers)
- RESEARCH-INTEGRATION-002: End-to-End Testing (depends on INTEGRATION-001)

### Validation Layer (Requires integration)
- RESEARCH-VALIDATION-001: Performance Testing (depends on INTEGRATION-001)
- RESEARCH-VALIDATION-002: Result Validation (depends on INTEGRATION-002)

### Documentation Layer (Can execute in parallel with validation)
- RESEARCH-DOCS-001: Code Documentation (depends on component completion)
- RESEARCH-DOCS-002: User Documentation (depends on INTEGRATION-002)
```

## Validation Checkpoints

### AI Self-Assessment Checklist
- [ ] All specification requirements mapped to tickets
- [ ] Ticket sizes are appropriate (2-6 hours each)
- [ ] Dependencies form logical implementation sequence
- [ ] AI collaboration guidance is specific and actionable
- [ ] Acceptance criteria are testable and complete
- [ ] Risk mitigation embedded in ticket structure

### Human Review Checklist
- [ ] **Coverage Completeness:** All specification components have corresponding tickets
- [ ] **Effort Realism:** Time estimates account for learning curve and debugging
- [ ] **Dependency Logic:** Execution sequence is practical and efficient
- [ ] **Quality Standards:** Acceptance criteria align with project quality goals
- [ ] **AI Integration:** Collaboration patterns match team capabilities and tools

### Ticket Quality Gates
- [ ] **Atomic Scope:** Each ticket has single, clear responsibility
- [ ] **Measurable Outcomes:** Acceptance criteria are objective and verifiable
- [ ] **Implementation Ready:** Sufficient detail for immediate execution
- [ ] **Integration Aligned:** Tickets connect logically to form complete system
- [ ] **Risk Managed:** High-risk elements broken down with validation steps

## Exit Criteria
- [ ] Complete set of structured tickets covering all implementation requirements
- [ ] Execution sequence with realistic timeline and dependencies
- [ ] AI collaboration guidance for each ticket type and complexity level
- [ ] Quality validation approach embedded in ticket acceptance criteria
- [ ] Implementation plan ready for systematic execution

---

# Stage 5: Implementation
**Duration:** 1-10 days (main work)  
**AI Persona:** Coding Partner  
**Primary Goal:** Execute tickets systematically to build working POC with AI assistance and continuous validation

## Entry Criteria
- [ ] Stage 4 completed with structured ticket list
- [ ] Execution sequence and dependencies defined
- [ ] Development environment set up and tested
- [ ] Success criteria and validation approach established

## High-Level Objective
Execute the planned tickets systematically using AI collaboration to build, test, and validate the working POC while maintaining quality and capturing learning throughout the process.

## AI Collaboration Pattern: Variable Autonomy Based on Task Type

### Execution Strategy
Implementation follows a structured approach with continuous AI collaboration:

1. **Ticket Selection:** Choose next ticket based on dependencies and priorities
2. **AI Collaboration:** Use appropriate AI tools and patterns for ticket type
3. **Implementation:** Build/code with AI assistance and human oversight
4. **Validation:** Test and verify against acceptance criteria
5. **Integration:** Ensure compatibility with existing components
6. **Documentation:** Update code and project documentation
7. **Progress Tracking:** Update status and plan next steps

### AI Collaboration Patterns by Ticket Type

#### Setup & Environment Tickets (Medium Autonomy)
```markdown
**AI Role:** Configuration Assistant
**Collaboration Pattern:** AI generates configurations, human validates and customizes

**Typical AI Assistance:**
- Generate project structure and configuration files
- Suggest dependency versions and compatibility solutions
- Create initial documentation templates
- Provide setup troubleshooting and debugging help

**Human Oversight:**
- Validate configurations match project requirements
- Customize settings for specific development environment
- Test setup completeness and functionality
- Approve and commit configuration changes
```

#### Data Pipeline Tickets (High Autonomy)
```markdown
**AI Role:** Data Processing Specialist
**Collaboration Pattern:** AI implements data processing logic, human validates correctness

**Typical AI Assistance:**
- Generate data loading and preprocessing code
- Create data validation and error handling logic
- Implement data transformations and feature extraction
- Generate comprehensive unit tests for data functions

**Human Oversight:**
- Validate data processing logic against requirements
- Test with real data and edge cases
- Verify data quality and transformation correctness
- Ensure error handling covers practical scenarios
```

#### Algorithm Implementation Tickets (High Autonomy with Mathematical Validation)
```markdown
**AI Role:** Mathematical Implementation Assistant
**Collaboration Pattern:** AI codes mathematical formulations, human validates algorithmic correctness

**Typical AI Assistance:**
- Translate mathematical notation to executable code
- Implement numerical computations with stability considerations
- Generate optimization strategies and performance improvements
- Create comprehensive tests for algorithm validation

**Human Oversight:**
- Verify mathematical implementations match paper specifications
- Validate numerical stability and edge case handling
- Test algorithmic correctness with known examples
- Ensure performance meets POC requirements
```

#### Integration Tickets (Medium Autonomy)
```markdown
**AI Role:** Integration Specialist
**Collaboration Pattern:** AI connects components, human validates architecture

**Typical AI Assistance:**
- Generate component interface implementations
- Create integration glue code and error handling
- Implement configuration and parameter management
- Generate integration tests and validation suites

**Human Oversight:**
- Validate integration architecture matches design
- Test end-to-end system functionality
- Verify error handling and recovery mechanisms
- Ensure system meets performance and quality requirements
```

### Implementation Workflow

#### Daily Execution Cycle
```yaml
## Morning Planning (15 minutes)
- Review available tickets and dependencies
- Select next tickets based on priority and readiness
- Plan AI collaboration strategy for selected work
- Set up development environment and context

## Implementation Blocks (2-4 hour focused work sessions)
### Block Structure:
- Ticket Analysis: Understand requirements and acceptance criteria
- AI Collaboration: Use appropriate AI tools and prompts
- Implementation: Code, build, and integrate with AI assistance
- Validation: Test against acceptance criteria and quality standards
- Documentation: Update code comments, docs, and progress tracking

## End-of-Day Review (15 minutes)
- Update ticket status and progress tracking
- Document any blockers or issues discovered
- Plan next day's work based on current state
- Capture learnings and insights from the day
```

#### AI Prompt Templates for Implementation

**Code Generation Prompt:**
```markdown
**Context:** Working on ticket [TICKET-ID]: [TITLE]
**Requirements:** [ACCEPTANCE_CRITERIA]
**Integration:** [COMPONENT_INTERFACES]

**Task:** Generate [SPECIFIC_FUNCTION/CLASS/MODULE] that meets the requirements.

**Requirements:**
- Functionality: [SPECIFIC_BEHAVIOR]
- Input/Output: [DATA_FORMATS_AND_INTERFACES]
- Error Handling: [EXPECTED_ERROR_SCENARIOS]
- Performance: [SPEED_AND_MEMORY_REQUIREMENTS]
- Testing: [VALIDATION_AND_TEST_REQUIREMENTS]

**Code Standards:**
- Style: [FORMATTING_AND_NAMING_CONVENTIONS]
- Documentation: [COMMENT_AND_DOCSTRING_REQUIREMENTS]
- Quality: [MAINTAINABILITY_AND_CLARITY_STANDARDS]

**Expected Output:**
- Working code implementation
- Comprehensive error handling
- Unit tests with good coverage
- Clear documentation and comments
```

**Debugging and Optimization Prompt:**
```markdown
**Context:** Debugging issue in [COMPONENT] while working on [TICKET-ID]
**Problem:** [SPECIFIC_ERROR_OR_PERFORMANCE_ISSUE]
**Current Code:** [RELEVANT_CODE_SNIPPET]
**Expected Behavior:** [WHAT_SHOULD_HAPPEN]
**Actual Behavior:** [WHAT_IS_HAPPENING]

**Analysis Needed:**
- Root cause identification
- Solution recommendations
- Performance optimization opportunities
- Testing strategies to prevent regression

**Constraints:**
- Must maintain interface compatibility
- Performance requirements: [SPECIFIC_NEEDS]
- Cannot break existing functionality
- Solution should be maintainable and clear
```

## Progress Tracking and Quality Assurance

### Ticket Execution Tracking
```yaml
## Active Ticket Status
TICKET-ID: [CURRENT_TICKET]
Status: [Not Started / In Progress / Testing / Done]
Progress: [PERCENTAGE_COMPLETE]
Time Invested: [ACTUAL_HOURS]
Estimated Remaining: [HOURS]
Blockers: [CURRENT_OBSTACLES]
Next Steps: [IMMEDIATE_ACTIONS]

## Daily Progress Log
Date: [DATE]
Tickets Completed: [LIST]
Tickets In Progress: [LIST]
Time Invested: [TOTAL_HOURS]
Key Achievements: [MAJOR_ACCOMPLISHMENTS]
Blockers Encountered: [ISSUES_AND_SOLUTIONS]
Learning Insights: [TECHNICAL_OR_PROCESS_LEARNINGS]
Tomorrow's Plan: [NEXT_PRIORITIES]
```

### Quality Validation During Implementation

#### Continuous Integration Checks
- [ ] All unit tests pass
- [ ] Code follows established style guidelines
- [ ] No security vulnerabilities detected
- [ ] Performance benchmarks within acceptable range
- [ ] Integration tests pass for affected components

#### Human Review Points
- [ ] **Algorithm Correctness:** Mathematical implementations match paper specifications
- [ ] **Data Quality:** Data processing produces expected and valid outputs
- [ ] **Integration Stability:** Components work together without interface conflicts
- [ ] **Error Handling:** System gracefully handles expected and unexpected failures
- [ ] **Documentation:** Code and user documentation are complete and accurate

## Risk Management During Implementation

### Common Implementation Challenges

**Algorithm Implementation Issues:**
- Mathematical formulations unclear or ambiguous
- Numerical stability problems with edge cases
- Performance significantly slower than expected
- Results don't match paper outcomes

**Risk Mitigation:**
- Implement simplified version first, then add complexity
- Use known test cases and validate intermediate results
- Profile code early and optimize bottlenecks systematically
- Document assumptions and limitations explicitly

**Data Pipeline Problems:**
- Data format different than expected
- Quality issues not caught by initial validation
- Preprocessing steps more complex than anticipated
- Memory or performance issues with large datasets

**Risk Mitigation:**
- Start with small, well-understood datasets
- Implement comprehensive data validation and logging
- Build pipeline incrementally with validation at each step
- Plan for data quality issues and alternative preprocessing approaches

**Integration Difficulties:**
- Component interfaces don't align as designed
- Performance degradation when components combined
- Error handling doesn't work across component boundaries
- Configuration management becomes complex

**Risk Mitigation:**
- Test component interfaces early with mock data
- Monitor performance at integration points
- Implement centralized error handling and logging
- Keep configuration simple and well-documented

### Contingency Planning

**If Implementation Significantly Behind Schedule:**
1. **Scope Reduction:** Identify minimum viable POC that still demonstrates core concepts
2. **Component Simplification:** Implement basic versions of complex components
3. **Alternative Approaches:** Consider different implementation strategies that are faster
4. **External Resources:** Use existing libraries or frameworks instead of custom implementation

**If Results Don't Match Paper:**
1. **Assumption Validation:** Review all assumptions made during implementation
2. **Implementation Verification:** Systematically verify each component against paper specifications
3. **Data Validation:** Ensure input data matches paper's dataset characteristics
4. **Scope Adjustment:** Focus on demonstrating approach rather than exact replication

## Deliverables

### 1. Working POC System
```yaml
## System Components
Data Pipeline: [STATUS] - [DESCRIPTION_OF_FUNCTIONALITY]
Core Algorithm: [STATUS] - [DESCRIPTION_OF_FUNCTIONALITY]
Validation Module: [STATUS] - [DESCRIPTION_OF_FUNCTIONALITY]
Integration Layer: [STATUS] - [DESCRIPTION_OF_FUNCTIONALITY]
User Interface: [STATUS] - [DESCRIPTION_OF_FUNCTIONALITY]

## Functionality Demonstration
End-to-End Execution: [DESCRIPTION_OF_COMPLETE_WORKFLOW]
Sample Results: [EXAMPLES_OF_SYSTEM_OUTPUT]
Performance Characteristics: [SPEED_MEMORY_SCALABILITY_MEASUREMENTS]
Quality Metrics: [ACCURACY_RELIABILITY_MEASURES]

## Known Limitations
Implementation Simplifications: [WHAT_WAS_SIMPLIFIED_AND_WHY]
Data Constraints: [LIMITATIONS_ON_INPUT_DATA_OR_SCALE]
Performance Boundaries: [KNOWN_PERFORMANCE_LIMITATIONS]
Future Enhancement Opportunities: [CLEAR_PATHS_FOR_IMPROVEMENT]
```

### 2. Documentation Package
```yaml
## Technical Documentation
Architecture Overview: [SYSTEM_DESIGN_AND_COMPONENT_RELATIONSHIPS]
Implementation Notes: [KEY_TECHNICAL_DECISIONS_AND_RATIONALE]
API Documentation: [COMPONENT_INTERFACES_AND_USAGE]
Performance Analysis: [BENCHMARKS_AND_OPTIMIZATION_NOTES]

## User Documentation
Setup Instructions: [STEP_BY_STEP_INSTALLATION_AND_CONFIGURATION]
Usage Guide: [HOW_TO_RUN_THE_SYSTEM_AND_INTERPRET_RESULTS]
Troubleshooting: [COMMON_ISSUES_AND_SOLUTIONS]
Examples: [SAMPLE_INPUTS_AND_EXPECTED_OUTPUTS]

## Development Documentation
Code Organization: [PROJECT_STRUCTURE_AND_MODULE_DESCRIPTIONS]
Testing Guide: [HOW_TO_RUN_TESTS_AND_ADD_NEW_ONES]
Extension Guide: [HOW_TO_MODIFY_OR_EXTEND_THE_SYSTEM]
Lessons Learned: [KEY_INSIGHTS_AND_RECOMMENDATIONS_FOR_FUTURE_WORK]
```

### 3. Validation Results
```yaml
## Functional Validation
Core Algorithm Testing: [RESULTS_OF_ALGORITHM_CORRECTNESS_TESTS]
Data Pipeline Testing: [RESULTS_OF_DATA_PROCESSING_VALIDATION]
Integration Testing: [RESULTS_OF_END_TO_END_SYSTEM_TESTS]
Error Handling Testing: [RESULTS_OF_FAILURE_SCENARIO_TESTS]

## Performance Validation
Speed Benchmarks: [EXECUTION_TIME_MEASUREMENTS_AND_ANALYSIS]
Memory Usage: [MEMORY_CONSUMPTION_ANALYSIS]
Scalability Assessment: [PERFORMANCE_WITH_DIFFERENT_DATA_SIZES]
Resource Utilization: [CPU_GPU_DISK_USAGE_CHARACTERISTICS]

## Quality Assessment
Code Quality Metrics: [TEST_COVERAGE_COMPLEXITY_MAINTAINABILITY]
Documentation Completeness: [COVERAGE_AND_CLARITY_ASSESSMENT]
Usability Evaluation: [EASE_OF_SETUP_AND_USE_ASSESSMENT]
Learning Value: [ASSESSMENT_OF_EDUCATIONAL_AND_DEMONSTRATION_VALUE]
```

## Exit Criteria
- [ ] Working POC demonstrates core concepts from research paper
- [ ] All critical tickets completed with acceptance criteria met
- [ ] System runs end-to-end without manual intervention
- [ ] Results are interpretable and align with expected outcomes
- [ ] Documentation sufficient for others to understand and use system
- [ ] Validation confirms system meets defined success criteria
- [ ] Learning objectives achieved and captured for future reference

---

## Workflow Summary

This 5-stage workflow provides a systematic approach to transforming research papers into working prototypes:

**Stage 1 (Discovery):** Rapid feasibility assessment and go/no-go decision
**Stage 2 (Investigation):** Architecture design and risk mitigation through targeted experiments  
**Stage 3 (Specification):** Detailed implementation planning with clear requirements
**Stage 4 (Planning):** Systematic task breakdown with AI collaboration guidance
**Stage 5 (Implementation):** Execution with continuous validation and quality assurance

Each stage builds on the previous one while maintaining flexibility to adapt based on discoveries and changing requirements. The structured AI collaboration patterns ensure efficient development while maintaining human oversight for critical decisions and quality validation.

The workflow is designed to be:
- **Time-bounded:** Clear duration expectations prevent endless perfectionism
- **Risk-managed:** Early decision points and mitigation strategies reduce failure probability
- **Learning-focused:** Captures insights and builds transferable knowledge
- **Quality-assured:** Built-in validation ensures working, demonstrable outcomes
- **AI-enhanced:** Systematic collaboration patterns accelerate development while maintaining quality